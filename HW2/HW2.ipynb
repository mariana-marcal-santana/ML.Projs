{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Machine Learning Homework 1\n",
    "Done by:\n",
    "Mariana Santana 106992\n",
    "Pedro Leal 106154\n",
    "LEIC-A\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. Programming\n",
    "#### Consider the heart-disease.csv dataset available at the course webpage‚Äôs homework tab. Using sklearn, apply a 5-fold stratified cross-validation with shuffling (random_state=0) for the assessment of predictive models along this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "General imports and variables for all exercises; run this cell before any other\n",
    "\"\"\"\n",
    "import pandas as pd, matplotlib.pyplot as plt, numpy as np\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "data = pd.read_csv('heart-disease.csv')\n",
    "\n",
    "X = data.drop(columns='target')\n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Compare the performance of a ùëòùëÅùëÅ with k=5 with and a na√Øve Bayes with Gaussian assumption (consider all remaining parameters as default):\n",
    "#### a. [1.0v] Plot two boxplots with the fold accuracies for each classifier. Is there one more stable than the other regarding performance? Why do you think that is the case? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "classifiers = [KNeighborsClassifier(n_neighbors=5), GaussianNB()]\n",
    "classifier_names = ['kNN (k=5)', 'Naive Bayes (Gaussian)']\n",
    "\n",
    "classifier_accuracies = []\n",
    "for classifier in classifiers:\n",
    "    accuracies = cross_val_score(classifier, X, y, cv=stratified_kfold)\n",
    "    classifier_accuracies.append(accuracies)\n",
    "\n",
    "plt.boxplot(classifier_accuracies, patch_artist=True)\n",
    "plt.xticks(np.arange(1, len(classifier_names) + 1), classifier_names)\n",
    "plt.xlabel('Classifier')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('5-fold Stratified Cross-Validation (with Shuffling)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The boxplot shows that the Naive Bayes classifier is more stable compared to the kNN classifier. The stability is evident from the smaller spread (interquartile range) and the absence of extreme outliers in the accuracy distribution of Naive Bayes. In contrast, the kNN classifier exhibits a larger variance in performance, as indicated by the wider range of accuracy values.\n",
    "\n",
    "Why is Naive Bayes More Stable?\n",
    "Nature of the Classifier:\n",
    "\n",
    "Naive Bayes makes strong assumptions about the independence of features and uses probability distributions. This tends to result in a consistent performance, even with different train-test splits.\n",
    "Impact of Parameter Selection:\n",
    "\n",
    "The stability of kNN can be influenced significantly by the choice of the k parameter and the nature of the data. When the value of k is fixed (e.g., k=5), the classifier's sensitivity to slight changes in the data is high. This can cause more variation in the accuracy across different folds.\n",
    "Effect of Data Distribution:\n",
    "\n",
    "If the dataset has overlapping classes or noisy data points, kNN might be more sensitive, as it directly uses the distances between data points to classify. Naive Bayes, on the other hand, relies on estimated distributions, making it less sensitive to such variations.\n",
    "Conclusion:\n",
    "Naive Bayes is more stable because of its probabilistic nature and less dependence on specific data points, whereas kNN's performance is highly variable due to its sensitivity to neighborhood structure in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. [1.0v] Report the accuracy of both models, this time scaling the data with a Min-Max scaler before training the models. Explain the impact that this preprocessing step has on the performance of each model, providing an explanation for the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "classifier_accuracies_scaled = []\n",
    "for classifier in classifiers:\n",
    "    accuracies = cross_val_score(classifier, X_scaled, y, cv=stratified_kfold)\n",
    "    classifier_accuracies_scaled.append(accuracies)\n",
    "\n",
    "for name, accuracies in zip(classifier_names, classifier_accuracies_scaled):\n",
    "    print(f'{name} Mean Accuracy: {np.mean(accuracies):.4f}')\n",
    "\n",
    "plt.boxplot(classifier_accuracies_scaled, patch_artist=True)\n",
    "plt.xticks(np.arange(1, len(classifier_names) + 1), classifier_names)\n",
    "plt.xlabel('Classifier')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('5-fold Stratified Cross-Validation with Min-Max Scaling')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impact of Min-Max Scaling on Each Model\n",
    "kNN Classifier:\n",
    "\n",
    "Impact: The performance of the kNN model generally improves after scaling. This is because kNN relies on the Euclidean distance between points. When the features are not scaled, attributes with larger numerical ranges dominate the distance calculations, making the kNN classifier biased towards certain features.\n",
    "Result: Scaling ensures that each feature contributes equally to the distance computation, leading to more meaningful neighborhoods and potentially improved classification accuracy.\n",
    "Naive Bayes (Gaussian):\n",
    "\n",
    "Impact: The performance of Naive Bayes might not show a significant change after scaling. This is because Gaussian Naive Bayes works by assuming a normal distribution for each feature. The Min-Max scaling does not change the overall structure of a feature's distribution, just its range.\n",
    "Result: Scaling typically has a minimal effect on Naive Bayes' performance, as it only shifts the values without altering the fundamental distribution used by the model.\n",
    "Explanation of the Results\n",
    "kNN Performance Increase: With Min-Max scaling, the kNN model's performance improves because it now measures distances in a fair and balanced manner for all features. When features have varying scales, the ones with a larger range dominate the distance calculations, skewing the classifier's behavior.\n",
    "\n",
    "Naive Bayes Stability: Gaussian Naive Bayes is relatively insensitive to the scale of the data since it only cares about the probability distributions of the features. Thus, scaling has little impact on its overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. [1.0v] Using scipy, test the hypothesis ‚Äúthe model kNN is statistically superior to Na√Øve Bayes regarding accuracy‚Äù, asserting whether it is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kNN_accuracies = cross_val_score(classifiers[0], X_scaled, y, cv=stratified_kfold)\n",
    "NB_accuracies = cross_val_score(classifiers[1], X_scaled, y, cv=stratified_kfold)\n",
    "\n",
    "t_stat, p_value = ttest_rel(kNN_accuracies, NB_accuracies, alternative='greater')\n",
    "\n",
    "print(f\"kNN Mean Accuracy: {np.mean(kNN_accuracies):.4f}\")\n",
    "print(f\"Naive Bayes Mean Accuracy: {np.mean(NB_accuracies):.4f}\")\n",
    "print(f\"t-statistic: {t_stat:.4f}, p-value: {p_value:.4f}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis. kNN is statistically superior to Naive Bayes in terms of accuracy.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. kNN is not statistically superior to Naive Bayes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Using a 80-20 train-test split, vary the number of neighbors of a ùëòùëÅùëÅ classifier using ùëò = {1, 5, 10, 20, 30}. Additionally, for each k, train one classifier using uniform weights and distance weights.\n",
    "#### a. [1.0v] Plot the train and test accuracy for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Define k values to test\n",
    "k_values = [1, 5, 10, 20, 30]\n",
    "\n",
    "# Store accuracies for plotting\n",
    "train_accuracies_uniform = []\n",
    "test_accuracies_uniform = []\n",
    "train_accuracies_distance = []\n",
    "test_accuracies_distance = []\n",
    "\n",
    "# Train and evaluate kNN classifiers for each k with different weights\n",
    "for k in k_values:\n",
    "    # kNN with uniform weights\n",
    "    knn_uniform = KNeighborsClassifier(n_neighbors=k, weights='uniform')\n",
    "    knn_uniform.fit(X_train, y_train)\n",
    "    \n",
    "    # kNN with distance weights\n",
    "    knn_distance = KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
    "    knn_distance.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate train and test accuracy for uniform weights\n",
    "    train_accuracy_uniform = accuracy_score(y_train, knn_uniform.predict(X_train))\n",
    "    test_accuracy_uniform = accuracy_score(y_test, knn_uniform.predict(X_test))\n",
    "    \n",
    "    # Calculate train and test accuracy for distance weights\n",
    "    train_accuracy_distance = accuracy_score(y_train, knn_distance.predict(X_train))\n",
    "    test_accuracy_distance = accuracy_score(y_test, knn_distance.predict(X_test))\n",
    "    \n",
    "    # Append accuracies for plotting\n",
    "    train_accuracies_uniform.append(train_accuracy_uniform)\n",
    "    test_accuracies_uniform.append(test_accuracy_uniform)\n",
    "    train_accuracies_distance.append(train_accuracy_distance)\n",
    "    test_accuracies_distance.append(test_accuracy_distance)\n",
    "\n",
    "# Plot the train and test accuracies for both weighting schemes\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot for uniform weights\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_values, train_accuracies_uniform, marker='o', linestyle='-', color='blue', label='Train Accuracy')\n",
    "plt.plot(k_values, test_accuracies_uniform, marker='o', linestyle='--', color='orange', label='Test Accuracy')\n",
    "plt.title('kNN with Uniform Weights')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot for distance weights\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_values, train_accuracies_distance, marker='o', linestyle='-', color='blue', label='Train Accuracy')\n",
    "plt.plot(k_values, test_accuracies_distance, marker='o', linestyle='--', color='orange', label='Test Accuracy')\n",
    "plt.title('kNN with Distance Weights')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plots\n",
    "plt.suptitle('Train and Test Accuracy for kNN Models with Different k Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. [1.5v] Explain the impact of increasing the neighbors on the generalization ability of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis\n",
    "a) Explanation of the Plot:\n",
    "The plots show the training and test accuracies for both uniform and distance weighting strategies as a function of the number of neighbors (k).\n",
    "The solid lines represent training accuracies, and the dashed lines represent testing accuracies.\n",
    "b) Impact of Increasing the Number of Neighbors (k):\n",
    "General Trends:\n",
    "\n",
    "As the number of neighbors (k) increases, the training accuracy decreases for both weighting strategies.\n",
    "This is expected because a larger k means that the model considers a broader neighborhood, leading to a smoother decision boundary that is less overfitted to the training data.\n",
    "Test accuracy initially increases and then may plateau or even decrease.\n",
    "With a small k, the model can overfit (high training accuracy but lower test accuracy).\n",
    "As k increases, the model generalizes better, but after a certain point, using too many neighbors can cause underfitting, reducing the test accuracy.\n",
    "Comparison Between Uniform and Distance Weights:\n",
    "\n",
    "For uniform weights, all neighbors contribute equally, which can lead to instability when the neighbors have varying distances from the query point.\n",
    "For distance weights, closer neighbors have a higher impact on the decision, which typically improves generalization and robustness.\n",
    "As a result, distance-weighted models often show better test accuracy and are less prone to overfitting compared to uniform-weighted models.\n",
    "Conclusion:\n",
    "Increasing k typically improves generalization up to a certain point, after which too many neighbors cause underfitting.\n",
    "Using distance weights can mitigate the issues with high values of k, leading to better overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) [1.5v] Considering the unique properties of the heart-disease.csv dataset, identify two possible difficulties of the na√Øve Bayes model used in the previous exercises when learning from the given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heart-disease dataset poses some specific challenges for a na√Øve Bayes model due to its unique properties. Here are two difficulties that a na√Øve Bayes model might face:\n",
    "\n",
    "1. Feature Independence Assumption:\n",
    "The core assumption of a na√Øve Bayes model is that all features are independent of each other, given the class label. However, in medical datasets like heart-disease, many features are interdependent. For example:\n",
    "\n",
    "Blood pressure (trestbps), cholesterol level (chol), and age (age) are often correlated.\n",
    "Similarly, features like thalach (maximum heart rate) and exang (exercise-induced angina) can be closely linked to each other and to the presence of heart disease.\n",
    "This violation of the independence assumption can lead to suboptimal performance, as the model might not capture the underlying relationships between these attributes effectively.\n",
    "\n",
    "2. Handling of Continuous Variables:\n",
    "The dataset has a mixture of categorical and continuous variables (e.g., age, trestbps, chol, thalach, and oldpeak), which are not inherently suitable for the Gaussian distribution assumption commonly used in na√Øve Bayes for continuous data.\n",
    "\n",
    "If the continuous variables do not follow a normal distribution, the Gaussian na√Øve Bayes may perform poorly.\n",
    "Additionally, outliers or skewed distributions in features like cholesterol or age can lead to inaccurate probability estimates, reducing the overall classification performance.\n",
    "In summary, the na√Øve Bayes model's assumption of feature independence and its handling of continuous variables could limit its effectiveness on the heart-disease dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
